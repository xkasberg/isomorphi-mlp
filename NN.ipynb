{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/heart.data') as inF:\n",
    "    data = inF.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((303, 13), (303,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "for line in data:\n",
    "    line = line.split(',')\n",
    "    \n",
    "    features = line[:13]\n",
    "    label = line[13]\n",
    "    \n",
    "    clean_features = []\n",
    "    for x in features:\n",
    "        try:\n",
    "            x = float(x)\n",
    "            clean_features.append(x)\n",
    "        except:\n",
    "            clean_features.append(0)\n",
    "    assert len(clean_features) == len(features)\n",
    "    X.append(clean_features)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X)\n",
    "\n",
    "# 0 = No heart Disease, 1 = Heart Disease\n",
    "y = np.array([0 if float(x) == float(0) else 1 for x in y])\n",
    "\n",
    "\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Based Rule\n",
    "\n",
    "- Baseline Accuracy: always pick the most common class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Based Algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning Algorithms like: \n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- Neural Networks\n",
    "\n",
    "use gradient descent as an optimization technique and require input data to be scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(x, normal=True, min=0, max=0):\n",
    "    \"\"\"\n",
    "        Normalizes a numpy array\n",
    "        This can be useful in algorithms\n",
    "        that do not assume any distribution \n",
    "        of the data like Neural Networks.\n",
    "    \"\"\"\n",
    "    min= x.min()\n",
    "    max = x.max()\n",
    "    range = x.max() - x.min()\n",
    "    scaled = (x - min) / range\n",
    "    return scaled\n",
    "\n",
    "X_prime = scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11170213, 0.00177305, 0.00177305, ..., 0.00531915, 0.        ,\n",
       "        0.0106383 ],\n",
       "       [0.11879433, 0.00177305, 0.0070922 , ..., 0.0035461 , 0.00531915,\n",
       "        0.00531915],\n",
       "       [0.11879433, 0.00177305, 0.0070922 , ..., 0.0035461 , 0.0035461 ,\n",
       "        0.01241135],\n",
       "       ...,\n",
       "       [0.10106383, 0.00177305, 0.0070922 , ..., 0.0035461 , 0.00177305,\n",
       "        0.01241135],\n",
       "       [0.10106383, 0.        , 0.0035461 , ..., 0.0035461 , 0.00177305,\n",
       "        0.00531915],\n",
       "       [0.06737589, 0.00177305, 0.00531915, ..., 0.00177305, 0.        ,\n",
       "        0.00531915]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prime.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The shape of W depends on whether or not X's columns are features or Examples.\\\n",
    "We dereive the deimensions for the Bias and Weight matrix based on the input dimensions and required output dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`W` must have the same number as rows, as input (`X`) has features (columns) \\\n",
    "Matrix Addition doesn't change the shape of matrices, \\\n",
    "Therefore `b` must have the same dimensions as the output Matrix `y` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Multi Layer Perceptron\n",
    "EXAMPLES = X.shape[0]\n",
    "FEATURES = X.shape[1]\n",
    "CLASSES = 1\n",
    "\n",
    "NODES = 16\n",
    "LAYERS = 1\n",
    "\n",
    "INITWEIGHT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.int64)\n",
    "X_xor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases = np.ones((4, 1), dtype=np.int64)\n",
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_xor = np.append(X_xor, biases, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 1, 1],\n",
       "       [1, 0, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_xor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([[.351, -.097, .457],[1.076, -.165, -.165], [1.16, .542, -.331]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine = np.matmul(X_xor, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.16 ,  0.542, -0.331],\n",
       "       [ 2.236,  0.377, -0.496],\n",
       "       [ 1.511,  0.445,  0.126],\n",
       "       [ 2.587,  0.28 , -0.039]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b Shape: (16,)\n",
      "b:\n",
      "[0.22199317 0.87073231 0.20671916 0.91861091 0.48841119 0.61174386\n",
      " 0.76590786 0.51841799 0.2968005  0.18772123 0.08074127 0.7384403\n",
      " 0.44130922 0.15830987 0.87993703 0.27408646]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "b1 = np.random.uniform(low=0, high=INITWEIGHT, size=NODES)\n",
    "print(f\"b Shape: {b1.shape}\")\n",
    "print(f\"b:\\n{b1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying `X` and `W` must be equal to `b` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W shape: (13, 16)\n",
      "W\n",
      "[[0.77 0.02 0.63 0.75 0.5  0.22 0.2  0.76 0.17 0.09 0.69 0.95 0.   0.51\n",
      "  0.81 0.61]\n",
      " [0.72 0.29 0.92 0.71 0.54 0.14 0.37 0.67 0.44 0.43 0.62 0.51 0.65 0.6\n",
      "  0.81 0.52]\n",
      " [0.91 0.32 0.09 0.3  0.11 0.83 0.05 0.63 0.55 0.82 0.2  0.86 0.35 0.75\n",
      "  0.3  0.88]\n",
      " [0.33 0.17 0.39 0.09 0.82 0.15 0.38 0.94 0.99 0.46 0.83 0.25 0.6  0.9\n",
      "  0.53 0.59]\n",
      " [0.04 0.36 0.08 0.31 0.33 0.77 0.04 0.43 0.31 0.64 0.35 0.04 0.88 0.76\n",
      "  0.88 0.42]\n",
      " [0.61 0.51 0.6  0.26 0.3  0.03 0.3  0.24 0.56 0.57 0.48 0.29 0.06 0.98\n",
      "  0.34 0.5 ]\n",
      " [0.98 0.44 0.32 0.52 0.58 0.85 0.07 0.46 0.78 0.72 0.59 0.04 0.35 0.56\n",
      "  0.3  0.51]\n",
      " [0.67 0.16 0.05 0.34 0.11 0.18 0.89 0.37 0.22 0.75 0.11 0.74 0.47 0.6\n",
      "  0.15 0.18]\n",
      " [0.65 0.05 0.25 0.54 0.23 0.38 0.92 0.93 0.57 0.53 0.01 0.98 0.57 0.79\n",
      "  0.56 0.88]\n",
      " [0.58 0.71 0.15 0.43 0.69 0.1  0.44 0.17 0.51 0.82 0.09 0.8  0.57 0.59\n",
      "  0.2  0.44]\n",
      " [0.3  0.04 0.03 0.45 0.74 0.56 0.39 0.17 0.84 0.6  0.78 0.85 0.6  0.78\n",
      "  0.62 0.02]\n",
      " [0.75 0.18 0.46 0.51 0.48 0.84 0.17 0.01 0.85 0.74 0.46 0.42 0.12 0.34\n",
      "  0.09 0.72]\n",
      " [0.08 0.21 0.57 0.29 0.66 0.8  0.35 0.09 0.81 0.78 0.39 0.86 0.38 0.26\n",
      "  0.83 0.74]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "W1 = np.random.uniform(low=0, high=INITWEIGHT, size=(FEATURES, NODES)).round(2) \n",
    "print(f\"W shape: {W1.shape}\")\n",
    "print(f\"W\\n{W1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We multiply input with its corresponding weight and sum all of them. \\\n",
    "Because we have arranged X as examples by features, we multiply W by X. \\\n",
    "This Neuron Activation results in a linear Affine Transform of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 13)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine = np.matmul(X_prime, W1)+b1\n",
    "affine.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An activation function in a neural network defines how the weighted sum of the input is transformed into an output from a node. \n",
    "<br> This is known as <i>Neuron Transfer</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = np.maximum(affine, 0) #ReLU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y1 now becomes the input for the next layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Hidden Layers, the shape of W depends on the number of nodes in layer n-1, and layer n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 16)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 16)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "W2 = np.random.uniform(low=0, high=INITWEIGHT, size=(y1.shape[1], NODES)).round(2) \n",
    "b2 = np.random.uniform(low=0, high=INITWEIGHT, size=NODES).round(2)\n",
    "y2 = np.maximum(np.matmul(y1, W2)+b2, 0)\n",
    "y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(45)\n",
    "W3 = np.random.uniform(low=0, high=INITWEIGHT,size=(y2.shape[1], CLASSES)).round(2) \n",
    "b3 = np.random.uniform(low=0, high=INITWEIGHT, size=CLASSES).round(2) \n",
    "W3.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "        converts hidden units into confidence indices\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.378481143072389"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(-.496)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999996732270753"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(14.934)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.matmul(y2, W3)+b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = sigmoid(np.matmul(y2, W3)+b3)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Maximum Likelihood** \n",
    "\n",
    "<p>To estimate the error of a set weights in a neural network, we prefer functions that optimization algorithms like SGD can easily descend. </p>\n",
    "\n",
    "<p><i>Maximum Likelihood</i> seeks to find the optimum values for parameters by maximizing  a likelihood function derived from the training data. <br>We are minimizing the dissimilarity between the empirical distribution found in the training set and the model distribution, as defined by KL Divergence. <br>Minimizing KL Divergence corresponds exactly to minimizing the cross-entropy between the distributions. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function: Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H_p(q) = -\\frac{1}{N} \\sum_{i=1}^{N} y_i * \\log(p(y_i)) + (1-yi) * \\log(1-p(y_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We seek a set of model weights that minimize the difference between the model's predicted probability distribution and the distribution of probabilities in the training dataset.<br><br><i> This is called cross-entropy.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_logit, y):\n",
    "    \"\"\"\n",
    "        the choice of loss is directly related to activation function of a neural network\n",
    "    \"\"\"\n",
    "    \n",
    "    bce = y * np.log(y_logit) + (1 - y) * np.log(1 - y_logit)\n",
    "    return np.mean(-bce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/85/2xqlyvkx7zv15pvt22w8djbw0000gn/T/ipykernel_1406/1479146115.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  bce = y * np.log(y_logit) + (1 - y) * np.log(1 - y_logit)\n",
      "/var/folders/85/2xqlyvkx7zv15pvt22w8djbw0000gn/T/ipykernel_1406/1479146115.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  bce = y * np.log(y_logit) + (1 - y) * np.log(1 - y_logit)\n"
     ]
    }
   ],
   "source": [
    "loss = binary_cross_entropy(output, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 13)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prime[:16].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FeedForwardMultiLayerPerceptron' object has no attribute 'sigmoid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 143\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[39m# # Backpropagate error and store in neurons\u001b[39;00m\n\u001b[1;32m    112\u001b[0m     \u001b[39m# def backward(self):\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[39m#             neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[39m#     #def accuracy(self):\u001b[39;00m\n\u001b[1;32m    135\u001b[0m network \u001b[39m=\u001b[39m FeedForwardMultiLayerPerceptron(\n\u001b[1;32m    136\u001b[0m     x \u001b[39m=\u001b[39m X_prime,\n\u001b[1;32m    137\u001b[0m     y \u001b[39m=\u001b[39m y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m     output_nodes \u001b[39m=\u001b[39m CLASSES,\n\u001b[1;32m    141\u001b[0m )\n\u001b[0;32m--> 143\u001b[0m loss, probs \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39;49mforward()\n",
      "Cell \u001b[0;32mIn[35], line 96\u001b[0m, in \u001b[0;36mFeedForwardMultiLayerPerceptron.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m forward_pass \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layer[\u001b[39m0\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layer[\u001b[39m1\u001b[39m] \n\u001b[0;32m---> 96\u001b[0m forward_pass \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msigmoid(forward_pass)\n\u001b[1;32m     99\u001b[0m \u001b[39mfor\u001b[39;00m weight, bias \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_layers:\n\u001b[1;32m    100\u001b[0m     forward_pass \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(forward_pass, weight) \u001b[39m+\u001b[39m bias\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FeedForwardMultiLayerPerceptron' object has no attribute 'sigmoid'"
     ]
    }
   ],
   "source": [
    "np.random.seed(45)\n",
    "\n",
    "class FeedForwardMultiLayerPerceptron(object):\n",
    "    \"\"\"\n",
    "        Multi Layer Perceptron for classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        x, \n",
    "        y,\n",
    "        hidden_layers:int, \n",
    "        hidden_units:int, \n",
    "        output_nodes:int, \n",
    "        init_weight=.3, \n",
    "        round_n=2):\n",
    "        \"\"\"\n",
    "            initializes an MLP\n",
    "            x is an numpy array of training data\n",
    "            output_nodes is the number of classes\n",
    "        \"\"\"\n",
    "        #self.optimizer = \n",
    "        #self.network = list()\n",
    "\n",
    "\n",
    "        self.inputs = x\n",
    "        self.labels = y\n",
    "        self.examples = self.inputs.shape[0]\n",
    "        self.features = self.inputs.shape[1]\n",
    "\n",
    "        self.init_weight = init_weight\n",
    "        \n",
    "        self.hidden_nodes = hidden_units\n",
    "        self.output_nodes = output_nodes\n",
    "        \n",
    "        self.input_layer = (\n",
    "            np.random.uniform(low=0, high=self.init_weight, size=(self.features, self.hidden_nodes)).round(round_n),\n",
    "            np.random.uniform(low=0, high=self.init_weight, size=self.hidden_nodes).round(round_n)\n",
    "        )\n",
    "\n",
    "        self.hidden_layers = [\n",
    "            (\n",
    "                np.random.uniform(low=0, high=self.init_weight, size=(self.hidden_nodes, self.hidden_nodes)).round(round_n),\n",
    "                np.random.uniform(low=0, high=self.init_weight, size=self.hidden_nodes).round(round_n)\n",
    "            ) for layer in range(hidden_layers)\n",
    "        ]\n",
    "\n",
    "        self.output_layer = (\n",
    "            np.random.uniform(low=0, high=self.init_weight,size=(self.hidden_nodes, self.output_nodes)).round(round_n),\n",
    "            np.random.uniform(low=0, high=self.init_weight, size=self.output_nodes).round(round_n) \n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "        self.network = list()\n",
    "        self.network.append(self.input_layer)\n",
    "        for layer in self.hidden_layers:\n",
    "            self.network.append(layer)\n",
    "        self.network.append(self.output_layer)\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'\"input layer shape\":\\n{self.input_layer.shape},\\n\"hidden layers\":\\n{self.hidden_layers},\\n\"output layer\":{self.output_layer}\"' \n",
    "        \n",
    "\n",
    "    def ReLU_transfer(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "    # def sigmoid_derivative(self, x):\n",
    "    #     \"\"\"\n",
    "    #     \"\"\"\n",
    "\t#     return x * (1.0 - x)\n",
    "\n",
    "\n",
    "    def loss(self, y_logit, y):\n",
    "        \"\"\"\n",
    "            the choice of loss is directly related to activation function of a neural network\n",
    "            this loss function is binary cross entropy, as our activation function is sigmoid\n",
    "        \"\"\"\n",
    "        bce = y * np.log(y_logit) + (1 - y) * np.log(1 - y_logit)\n",
    "        return np.mean(-bce)\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        forward_pass = np.matmul(self.inputs, self.input_layer[0]) + self.input_layer[1] \n",
    "        forward_pass = self.sigmoid(forward_pass)\n",
    "\n",
    "\n",
    "        for weight, bias in self.hidden_layers:\n",
    "            forward_pass = np.matmul(forward_pass, weight) + bias\n",
    "            forward_pass = self.sigmoid(forward_pass)\n",
    "\n",
    "\n",
    "        forward_pass = np.matmul(forward_pass, self.output_layer[0]) + self.output_layer[1]\n",
    "\n",
    "        self.pass_probs = self.sigmoid(forward_pass)\n",
    "        self.pass_loss = self.loss(forward_pass, self.labels)\n",
    "        print(f\"Pass Loss: {self.pass_loss}\")\n",
    "        return self.pass_loss, self.pass_probs\n",
    "\n",
    "    # # Backpropagate error and store in neurons\n",
    "    # def backward(self):\n",
    "\n",
    "    #     for i in reversed(range(len(network))):\n",
    "    #         layer = network[i]\n",
    "    #         errors = list()\n",
    "    #         if i != len(network)-1:\n",
    "    #             for j in range(len(layer)):\n",
    "    #                 error = 0.0\n",
    "    #                 for neuron in network[i + 1]:\n",
    "    #                     error += (neuron['weights'][j] * neuron['delta'])\n",
    "    #                 errors.append(error)\n",
    "    #         else:\n",
    "    #             for j in range(len(layer)):\n",
    "    #                 neuron = layer[j]\n",
    "    #                 errors.append(neuron['output'] - expected[j])\n",
    "    #         for j in range(len(layer)):\n",
    "    #             neuron = layer[j]\n",
    "    #             neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "    #     #def accuracy(self):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "network = FeedForwardMultiLayerPerceptron(\n",
    "    x = X_prime,\n",
    "    y = y,\n",
    "    hidden_layers = 1,\n",
    "    hidden_units = 8,\n",
    "    output_nodes = CLASSES,\n",
    ")\n",
    "\n",
    "loss, probs = network.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('origami_datasci-2ZMWArRq')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb8348daee5f03dddbbbd8761b0688d0a17b168f6df1d4bcf8481e2ef54f7e76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
