{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Classification MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "<li> https://www.deeplearningbook.org/contents/mlp.html <br>\n",
    "<li> https://deepnotes.io/softmax-crossentropy <br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set Information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data downloaded from: https://archive.ics.uci.edu/ml/datasets/seeds\n",
    "\n",
    "The examined group comprised kernels belonging to three different varieties of wheat: Kama, Rosa and Canadian, 70 elements each, randomly selected for the experiment. High quality visualization of the internal kernel structure was detected using a soft X-ray technique. It is non-destructive and considerably cheaper than other more sophisticated imaging techniques like scanning microscopy or laser technology. The images were recorded on 13x18 cm X-ray KODAK plates. Studies were conducted using combine harvested wheat grain originating from experimental fields, explored at the Institute of Agrophysics of the Polish Academy of Sciences in Lublin. \n",
    "\n",
    "The data set can be used for the tasks of classification and cluster analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataSet:\n",
    "    \"\"\"\n",
    "       Isomorphi DataSet class\n",
    "       Takes a dataset, loads, scales, shuffles, splits & batches data\n",
    "       !!! Assumes all features are already Continuous Numeric Features !!! \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        self.filename = filename\n",
    "        self.splitsize = kwargs.get('splitsize')\n",
    "        self.valsize = kwargs.get('valsize')\n",
    "        self.batchsize = kwargs.get('batchsize')\n",
    "\n",
    "\n",
    "    def load(self, **kwargs):\n",
    "        \"\"\" \n",
    "            loads the dataset from the passed filename.\n",
    "            Assumes the dataset is in a .txt file\n",
    "            shuffles the dataset\n",
    "            returns X and y.\n",
    "        \"\"\"\n",
    "        with open(self.filename) as inF:\n",
    "            rawdata = inF.readlines()\n",
    "\n",
    "        data = []\n",
    "        for line in rawdata:\n",
    "            line = line.strip()\n",
    "            line = line.split('\\t')\n",
    "            line = [x for x in line if x != '']\n",
    "            data.append(line)\n",
    "\n",
    "        self.X = np.array(data).astype(float)\n",
    "        np.random.seed(kwargs.get('seed'))\n",
    "        np.random.shuffle(self.X)\n",
    "\n",
    "        self.splits = self.split(validation=kwargs.get('validation'), valsize=kwargs.get('valsize'))\n",
    "        self.datasets = [self.scale(dataset[:, :-1]) for dataset in self.splits]\n",
    "        self.labels = [dataset[:, -1].astype(int) - 1 for dataset in self.splits]\n",
    "        self.name()\n",
    "        return self.datasets, self.labels\n",
    "    \n",
    "\n",
    "    def scale(self, X, **kwargs):\n",
    "        \"\"\"\n",
    "            Normalizes a numpy array via Standard Normal Distribution\n",
    "            This can be useful in algorithms that do not assume any distribution,\n",
    "            and is a required steps for Networks that learn via Gradient Descent\n",
    "        \"\"\"\n",
    "        min = X.min()\n",
    "        max = X.max()\n",
    "        range = max - min\n",
    "        return (X - min) / range\n",
    "    \n",
    "\n",
    "    def split(self, testsize:float=.10, **kwargs):\n",
    "        \"\"\"\n",
    "            segregates the dataset into distinct splits\n",
    "        \"\"\"\n",
    "        validation_set = kwargs.get('validation')\n",
    "        if validation_set:\n",
    "            valsize = kwargs.get('valsize')\n",
    "            trainsize =  1 - testsize - valsize\n",
    "            train_n = round(self.X.shape[0] * trainsize)\n",
    "            val_n = round(self.X.shape[0] * valsize)\n",
    "            train = self.X[:train_n]\n",
    "            val = self.X[train_n:train_n+val_n]\n",
    "            test = self.X[train_n+val_n:]\n",
    "            return train, val, test\n",
    "        \n",
    "        else:\n",
    "            trainsize =  1 - testsize\n",
    "            train_n = round(self.X.shape[0] * trainsize)\n",
    "            train = self.X[:train_n]\n",
    "            test = self.X[train_n:]\n",
    "            return train, test\n",
    "    \n",
    "\n",
    "    def name(self):\n",
    "        \"\"\"\n",
    "            Assigns named attributes to specific datasets\n",
    "        \"\"\"\n",
    "        self.X_train = self.datasets[0]\n",
    "        self.y_train = self.labels[0]\n",
    "        if len(self.datasets) > 2:\n",
    "            self.X_val = self.datasets[1]\n",
    "            self.y_val = self.labels[1]\n",
    "            self.X_test = self.datasets[2]\n",
    "            self.y_test = self.labels[2]\n",
    "\n",
    "        else:\n",
    "            self.X_test = self.datasets[1]\n",
    "            self.y_test = self.labels[1]\n",
    "\n",
    "\n",
    "    def batch(self, dataset, labels, batch_size:int=1):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        num_batches = len(dataset) // batch_size\n",
    "        if len(dataset) % batch_size != 0:\n",
    "            num_batches += 1\n",
    "\n",
    "        batches = []\n",
    "        for i in range(num_batches):\n",
    "            start_index = i * batch_size\n",
    "            end_index = (i + 1) * batch_size\n",
    "            batch = (dataset[start_index:end_index], labels[start_index:end_index])\n",
    "            batches.append(batch)\n",
    "        \n",
    "        return batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168 training batches\n",
      "21 validation batches\n",
      "21 test batches\n"
     ]
    }
   ],
   "source": [
    "seeds = EmbeddingDataSet('data/seeds.data')\n",
    "seeds.load(validation=True, valsize=.10)\n",
    "train_batches = seeds.batch(seeds.X_train, seeds.y_train, 1)\n",
    "val_batches = seeds.batch(seeds.X_val, seeds.y_val)\n",
    "test_batches = seeds.batch(seeds.X_test, seeds.y_test)\n",
    "\n",
    "print(f\"{len(train_batches)} training batches\")\n",
    "print(f\"{len(val_batches)} validation batches\")\n",
    "print(f\"{len(test_batches)} test batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Network\n",
      "Epoch 0, loss: 0.7822\n",
      "Epoch 10, loss: 1.1614\n",
      "Epoch 20, loss: 1.4756\n",
      "Epoch 30, loss: 1.6693\n",
      "Epoch 40, loss: 1.9621\n",
      "Epoch 50, loss: 2.0988\n"
     ]
    }
   ],
   "source": [
    "class FFMLP(object):\n",
    "    \n",
    "    \"\"\"\n",
    "        Feed-Forward Multi Layer Perceptron for Classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        epochs:int,\n",
    "        features:int,\n",
    "        output_nodes:int=1,\n",
    "        hidden_layers:int=1, \n",
    "        hidden_nodes:int=16, \n",
    "        init_weight:float=1,\n",
    "        alpha:float=.01):\n",
    "        \"\"\"\n",
    "            initializes a Feed Forward Multilayer Perceptron\n",
    "            for classification tasks\n",
    "            \n",
    "            x is an numpy array of training data\n",
    "            output_nodes is the number of classes\n",
    "        \"\"\"\n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.params = {\n",
    "            \"epochs\" : epochs,\n",
    "            \"hidden layers\": hidden_layers,\n",
    "            \"hidden nodes\": hidden_nodes,\n",
    "            \"initialization weight\": init_weight,\n",
    "            \"learning rate\": alpha,\n",
    "        }\n",
    "        \n",
    "        self.hidden_layer = {\n",
    "            \"weights\": np.random.uniform(low=0, high=init_weight, size=(features, hidden_nodes)),\n",
    "            \"bias\": np.random.uniform(low=0, high=init_weight, size=hidden_nodes)\n",
    "        }\n",
    "\n",
    "        if hidden_layers > 1:\n",
    "            self.hidden_layers = [\n",
    "                {\n",
    "                    \"weights\": np.random.uniform(low=0, high=init_weight, size=(hidden_nodes, hidden_nodes)),\n",
    "                    \"bias\": np.random.uniform(low=0, high=init_weight, size=hidden_nodes)\n",
    "                } for layer in range(hidden_layers-1)\n",
    "            ]\n",
    "        else:\n",
    "            self.hidden_layers = []\n",
    "\n",
    "        self.output_layer = {\n",
    "            \"weights\": np.random.uniform(low=0, high=init_weight, size=(hidden_nodes, output_nodes)),\n",
    "            \"bias\": np.random.uniform(low=0, high=init_weight, size=output_nodes)\n",
    "        }\n",
    "\n",
    "        self.network = dict() # a data structure to hold our layers\n",
    "        self.network['hidden layer 1'] = self.hidden_layer\n",
    "        for idx, layer in enumerate(self.hidden_layers, start=2):\n",
    "            self.network[f'hidden layer {idx}'] = layer        \n",
    "        self.network['output layer'] = self.output_layer # hidden to output\n",
    "        print('Initialized Network')\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Parameters: {self.params}\\n, Network: {self.network}'\n",
    "        \n",
    "\n",
    "    def ReLU(self, x):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def delta_ReLU(self, x):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "    def delta_sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return x * (1.0 - x)   \n",
    "    \n",
    "\n",
    "    def softmax_stable(self, X):\n",
    "        \"\"\"\n",
    "            stable softmax\n",
    "        \"\"\"\n",
    "        exps = np.exp(X - np.max(X))\n",
    "        return exps / np.sum(exps)\n",
    "    \n",
    "\n",
    "    def cross_entropy(self, X, y):\n",
    "        \"\"\"\n",
    "        X is the output from fully connected layer (num_examples x num_classes)\n",
    "          after softmax()\n",
    "        y is labels (num_examples x 1)\n",
    "            Note that y is not one-hot encoded vector. \n",
    "            It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.\n",
    "        \"\"\"\n",
    "        p = X\n",
    "        m = y.shape[0]\n",
    "        # takes the log of the highest values in p (which is the probability of the class)\n",
    "        log_likelihood = -np.log(p[range(m), y]) \n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def delta_cross_entropy(self, X, y):\n",
    "        \"\"\"\n",
    "        X is the output from fully connected layer (num_examples x num_classes)\n",
    "        y is labels (num_examples x 1)\n",
    "            Note that y is not one-hot encoded vector. \n",
    "            It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        grad = X.copy()\n",
    "        grad[range(m), y] -= 1\n",
    "        grad = grad / m\n",
    "        return grad\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        forward = batch[0]\n",
    "        for key, value in self.network.items():\n",
    "            self.network[key]['z'] = forward # inputs\n",
    "            forward = np.dot(forward, value['weights']) + value['bias']\n",
    "            forward = self.ReLU(forward) \n",
    "            self.network[key]['a'] = forward # activations\n",
    "            \n",
    "        outputs = np.apply_along_axis(self.softmax_stable, 1, forward)\n",
    "        loss = self.cross_entropy(outputs, batch[1])\n",
    "        return loss, outputs\n",
    "\n",
    "\n",
    "    def backward(self, batch, outputs):\n",
    "        \"\"\"\n",
    "            Naive backward propogation that involves \n",
    "            recomputation of the gradient\n",
    "            At every step\n",
    "        \"\"\"\n",
    "        grad = self.delta_cross_entropy(outputs, batch[1])\n",
    "\n",
    "        self.partials = []\n",
    "        for key in reversed(list(network.network.keys())):\n",
    "            z = network.network[key]['z']\n",
    "            db = np.sum(grad, axis=0)\n",
    "            dW = np.dot(z.T, grad) # the current layers inputs were the previous layers activations\n",
    "            self.partials.append([dW, db])\n",
    "            grad = np.dot(grad, network.network[key]['weights'].T) * self.delta_ReLU(z)\n",
    "\n",
    "\n",
    "    def fit(self, batches):\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch in batches:\n",
    "                pass_loss, outputs = self.forward(batch)\n",
    "                self.backward(batch, outputs)\n",
    "                for idx, key in enumerate(reversed(list(self.network.keys()))):\n",
    "                    self.network[key]['weights'] -= self.alpha * self.partials[idx][0]\n",
    "                    self.network[key]['bias'] -= self.alpha * self.partials[idx][1]\n",
    "\n",
    "            # Print loss every 10 epochs\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, loss: {pass_loss:.4f}\")\n",
    "\n",
    "# Multi Layer Perceptron\n",
    "EPOCHS = 60\n",
    "CLASSES = 3\n",
    "HIDDEN_NODES = 64\n",
    "HIDDEN_LAYERS = 1\n",
    "INITWEIGHT = 1\n",
    "ALPHA = .10\n",
    "\n",
    "\n",
    "#np.random.seed(45)\n",
    "network = FFMLP(\n",
    "    epochs=EPOCHS,\n",
    "    features=7,\n",
    "    hidden_layers = HIDDEN_LAYERS,\n",
    "    hidden_nodes = HIDDEN_NODES,\n",
    "    output_nodes = CLASSES,\n",
    "    init_weight=INITWEIGHT,\n",
    "    alpha=ALPHA\n",
    ")\n",
    "\n",
    "network.fit(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(batches):\n",
    "   predictions = []\n",
    "   for batch in batches:\n",
    "      _, batch_pred_probs = network.forward(batch)\n",
    "      predictions.append(np.argmax(batch_pred_probs))\n",
    "   return predictions\n",
    "\n",
    "train_predictions = predict(train_batches)\n",
    "val_predictions = predict(val_batches)\n",
    "test_predictions = predict(test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions:list, labels:list):\n",
    "   assert len(predictions) == len(labels)\n",
    "   total = 0\n",
    "   for pred, label in zip(predictions, labels):\n",
    "      if pred == label:\n",
    "         total += 1 \n",
    "\n",
    "   return total / len(predictions)\n",
    "\n",
    "train_accuracy = accuracy(train_predictions, seeds.y_train.tolist())\n",
    "val_accuracy = accuracy(val_predictions, seeds.y_val.tolist())\n",
    "test_accuracy = accuracy(test_predictions, seeds.y_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 89.29%\n",
      "validation accuracy: 76.19%\n",
      "test accuracy: 85.71%\n"
     ]
    }
   ],
   "source": [
    "print(f\"train accuracy: {round(train_accuracy, 4)*100}%\")\n",
    "print(f\"validation accuracy: {round(val_accuracy, 4)*100}%\")\n",
    "print(f\"test accuracy: {round(test_accuracy, 4)*100}%\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('origami_datasci-2ZMWArRq')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb8348daee5f03dddbbbd8761b0688d0a17b168f6df1d4bcf8481e2ef54f7e76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
