{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Classification MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "https://deepnotes.io/softmax-crossentropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set Information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data downloaded from: https://archive.ics.uci.edu/ml/datasets/seeds\n",
    "\n",
    "The examined group comprised kernels belonging to three different varieties of wheat: Kama, Rosa and Canadian, 70 elements each, randomly selected for the experiment. High quality visualization of the internal kernel structure was detected using a soft X-ray technique. It is non-destructive and considerably cheaper than other more sophisticated imaging techniques like scanning microscopy or laser technology. The images were recorded on 13x18 cm X-ray KODAK plates. Studies were conducted using combine harvested wheat grain originating from experimental fields, explored at the Institute of Agrophysics of the Polish Academy of Sciences in Lublin. \n",
    "\n",
    "The data set can be used for the tasks of classification and cluster analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[7.02039093e-01, 6.74059239e-01, 4.43750675e-03, ...,\n",
       "          1.30268312e-01, 1.13873099e-01, 2.14600575e-01],\n",
       "         [5.11579733e-01, 5.89628801e-01, 2.51327816e-03, ...,\n",
       "          9.88032476e-02, 3.29661591e-01, 2.03654071e-01],\n",
       "         [7.34927694e-01, 6.92221600e-01, 4.12825573e-03, ...,\n",
       "          1.28893863e-01, 2.34873698e-01, 2.12440727e-01],\n",
       "         ...,\n",
       "         [5.88156177e-01, 6.24480900e-01, 3.45575747e-03, ...,\n",
       "          1.10240627e-01, 2.29474077e-01, 2.03703158e-01],\n",
       "         [6.89767227e-01, 6.80440609e-01, 2.91579536e-03, ...,\n",
       "          1.20254469e-01, 6.48347225e-02, 2.22994532e-01],\n",
       "         [4.89981249e-01, 5.95028422e-01, 1.22718660e-04, ...,\n",
       "          9.03111163e-02, 2.28443240e-01, 2.15287800e-01]]),\n",
       "  array([[0.93709687, 0.80200981, 0.00665639, 0.27733579, 0.15776828,\n",
       "          0.11048781, 0.2672816 ],\n",
       "         [0.6184564 , 0.65042356, 0.00676982, 0.22211509, 0.12322312,\n",
       "          0.10636301, 0.20958603],\n",
       "         [0.63908038, 0.67310994, 0.00521271, 0.24175943, 0.11394233,\n",
       "          0.0046404 , 0.22123857],\n",
       "         [0.89584891, 0.79169782, 0.00577987, 0.2729532 , 0.14425957,\n",
       "          0.14714693, 0.27047832],\n",
       "         [0.94328406, 0.8087126 , 0.00620266, 0.2913601 , 0.15230292,\n",
       "          0.07645824, 0.27831543],\n",
       "         [0.62515919, 0.67053194, 0.00458884, 0.24387339, 0.11657188,\n",
       "          0.27944975, 0.23464416],\n",
       "         [0.81799339, 0.75766825, 0.00565097, 0.27264384, 0.1392067 ,\n",
       "          0.06573378, 0.26356929],\n",
       "         [0.54936607, 0.62361239, 0.00532099, 0.2187637 , 0.10749733,\n",
       "          0.09976334, 0.19808816],\n",
       "         [1.        , 0.83861737, 0.0055891 , 0.29636141, 0.15508716,\n",
       "          0.05903098, 0.27944975],\n",
       "         [0.72776348, 0.7117799 , 0.00598095, 0.24691543, 0.13240079,\n",
       "          0.01303951, 0.21608258],\n",
       "         [0.72982588, 0.70456151, 0.00698637, 0.23815024, 0.13647402,\n",
       "          0.01943294, 0.22288849],\n",
       "         [0.77932343, 0.72879468, 0.00689356, 0.25021526, 0.14137222,\n",
       "          0.        , 0.22304317],\n",
       "         [0.53956968, 0.61175361, 0.00613048, 0.21329834, 0.10899257,\n",
       "          0.077541  , 0.20303791],\n",
       "         [0.64217397, 0.67414114, 0.00530552, 0.23871739, 0.11884052,\n",
       "          0.17488618, 0.22288849],\n",
       "         [0.68806233, 0.68754673, 0.00649655, 0.24000639, 0.13080243,\n",
       "          0.09976334, 0.21835122],\n",
       "         [0.53956968, 0.62464359, 0.00443416, 0.22556961, 0.10466153,\n",
       "          0.18354825, 0.2185059 ],\n",
       "         [0.64732997, 0.67929713, 0.00496007, 0.24624515, 0.1189952 ,\n",
       "          0.32327571, 0.24103759],\n",
       "         [0.93400327, 0.79891621, 0.0067956 , 0.2742422 , 0.15462312,\n",
       "          0.04557384, 0.2755312 ],\n",
       "         [0.95617405, 0.81593099, 0.00600158, 0.2874931 , 0.15699488,\n",
       "          0.13982542, 0.28218243],\n",
       "         [0.70146791, 0.70249911, 0.00554269, 0.24768883, 0.12312   ,\n",
       "          0.03603525, 0.2338192 ],\n",
       "         [0.70456151, 0.70301471, 0.00567159, 0.24851378, 0.12925563,\n",
       "          0.16550227, 0.22577585]]),\n",
       "  array([[0.72936637, 0.76486884, 0.00313702, 0.26661196, 0.13187715,\n",
       "          0.30188163, 0.26183949],\n",
       "         [0.65894342, 0.75206467, 0.        , 0.26690296, 0.11272909,\n",
       "          0.23378672, 0.25677603],\n",
       "         [0.61296481, 0.702012  , 0.00299734, 0.2470565 , 0.11878197,\n",
       "          0.14776596, 0.24414646],\n",
       "         [0.89756663, 0.83645581, 0.00447564, 0.29472294, 0.15504106,\n",
       "          0.17232669, 0.29763297],\n",
       "         [0.61820288, 0.7171442 , 0.0014783 , 0.25421519, 0.11121587,\n",
       "          0.08223188, 0.25165436],\n",
       "         [1.        , 0.87603234, 0.00530209, 0.29577055, 0.16796163,\n",
       "          0.07332716, 0.29268591],\n",
       "         [0.77709101, 0.7910592 , 0.0029275 , 0.28226797, 0.13507819,\n",
       "          0.13176075, 0.25421519],\n",
       "         [0.62111292, 0.71248814, 0.00228147, 0.25444799, 0.1134857 ,\n",
       "          0.29495574, 0.24408826],\n",
       "         [0.89523859, 0.83529179, 0.00446982, 0.2924531 , 0.15207282,\n",
       "          0.00552325, 0.26183949],\n",
       "         [0.8585721 , 0.81957758, 0.0043185 , 0.28913566, 0.15131621,\n",
       "          0.24234223, 0.29326792],\n",
       "         [0.9330691 , 0.86497419, 0.00315448, 0.30205623, 0.15573947,\n",
       "          0.2250566 , 0.29501394],\n",
       "         [0.97439166, 0.86439218, 0.00529627, 0.29000867, 0.16772883,\n",
       "          0.26527334, 0.28244257],\n",
       "         [0.64090118, 0.73285841, 0.00112328, 0.2622469 , 0.11459152,\n",
       "          0.21318364, 0.25433159],\n",
       "         [0.64381122, 0.71074212, 0.00415554, 0.25165436, 0.12483486,\n",
       "          0.16231616, 0.25165436],\n",
       "         [0.94936532, 0.85799009, 0.00472008, 0.29344252, 0.16051193,\n",
       "          0.11930578, 0.28738964],\n",
       "         [0.89349257, 0.84518592, 0.0032767 , 0.29315151, 0.1505596 ,\n",
       "          0.20125248, 0.29024148],\n",
       "         [0.69211787, 0.73344042, 0.00461532, 0.2546226 , 0.13286656,\n",
       "          0.44511375, 0.24397185],\n",
       "         [0.85915411, 0.83470978, 0.00259575, 0.29751657, 0.14101467,\n",
       "          0.10661801, 0.29513034],\n",
       "         [0.95751343, 0.86846623, 0.00396929, 0.30089222, 0.16214156,\n",
       "          0.21714129, 0.29000867],\n",
       "         [0.71365216, 0.76312282, 0.00232221, 0.27144262, 0.12722109,\n",
       "          0.26166489, 0.26696116],\n",
       "         [0.94703729, 0.84809596, 0.00580844, 0.2923949 , 0.16732143,\n",
       "          0.12500946, 0.27214103]])],\n",
       " [array([0, 2, 0, 2, 0, 1, 0, 2, 2, 2, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0,\n",
       "         1, 2, 0, 2, 0, 2, 0, 1, 1, 1, 1, 0, 0, 1, 2, 1, 0, 2, 0, 2, 1, 2,\n",
       "         1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 0, 0, 2, 2, 0, 2, 1, 0, 1, 2, 0, 2,\n",
       "         0, 2, 0, 1, 2, 1, 1, 0, 0, 0, 2, 2, 0, 2, 0, 2, 0, 1, 1, 2, 1, 2,\n",
       "         1, 0, 1, 1, 1, 2, 2, 1, 2, 2, 0, 2, 1, 2, 2, 0, 1, 1, 0, 2, 2, 2,\n",
       "         1, 2, 2, 1, 2, 1, 0, 1, 2, 0, 1, 1, 2, 0, 2, 1, 0, 2, 1, 1, 0, 0,\n",
       "         1, 1, 2, 1, 2, 0, 0, 1, 1, 2, 1, 1, 0, 1, 0, 2, 1, 2, 1, 1, 2, 0,\n",
       "         0, 0, 2, 1, 1, 0, 0, 1, 0, 0, 0, 2, 0, 2]),\n",
       "  array([1, 2, 0, 1, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 0, 0]),\n",
       "  array([2, 2, 2, 1, 2, 1, 0, 2, 0, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 0])])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingDataSet:\n",
    "    \"\"\"\n",
    "       Isomorphi DataSet class\n",
    "       Takes a dataset, loads, scales, shuffles, splits & batches data\n",
    "       !!! Assumes all features are already Continuous Numeric Features !!! \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        self.filename = filename\n",
    "        self.splitsize = kwargs.get('splitsize')\n",
    "        self.valsize = kwargs.get('valsize')\n",
    "        self.batchsize = kwargs.get('batchsize')\n",
    "\n",
    "\n",
    "    def load(self, **kwargs):\n",
    "        \"\"\" \n",
    "            loads the dataset from the passed filename.\n",
    "            Assumes the dataset is in a .txt file\n",
    "            shuffles the dataset\n",
    "            returns X and y.\n",
    "        \"\"\"\n",
    "        with open(self.filename) as inF:\n",
    "            rawdata = inF.readlines()\n",
    "\n",
    "        data = []\n",
    "        for line in rawdata:\n",
    "            line = line.strip()\n",
    "            line = line.split('\\t')\n",
    "            line = [x for x in line if x != '']\n",
    "            data.append(line)\n",
    "\n",
    "        self.X = np.array(data).astype(float)\n",
    "        np.random.seed(kwargs.get('seed'))\n",
    "        np.random.shuffle(self.X)\n",
    "\n",
    "        self.splits = self.split(validation=kwargs.get('validation'), valsize=kwargs.get('valsize'))\n",
    "        self.datasets = [self.scale(dataset[:, :-1]) for dataset in self.splits]\n",
    "        self.labels = [dataset[:, -1].astype(int) - 1 for dataset in self.splits]\n",
    "        self.name()\n",
    "        return self.datasets, self.labels\n",
    "    \n",
    "\n",
    "    def scale(self, X, **kwargs):\n",
    "        \"\"\"\n",
    "            Normalizes a numpy array via Standard Normal Distribution\n",
    "            This can be useful in algorithms that do not assume any distribution,\n",
    "            and is a required steps for Networks that learn via Gradient Descent\n",
    "        \"\"\"\n",
    "        min = X.min()\n",
    "        max = X.max()\n",
    "        range = max - min\n",
    "        return (X - min) / range\n",
    "    \n",
    "\n",
    "    def split(self, testsize:float=.10, **kwargs):\n",
    "        \"\"\"\n",
    "            segregates the dataset into distinct splits\n",
    "        \"\"\"\n",
    "        validation_set = kwargs.get('validation')\n",
    "        if validation_set:\n",
    "            valsize = kwargs.get('valsize')\n",
    "            trainsize =  1 - testsize - valsize\n",
    "            train_n = round(self.X.shape[0] * trainsize)\n",
    "            val_n = round(self.X.shape[0] * valsize)\n",
    "            train = self.X[:train_n]\n",
    "            val = self.X[train_n:train_n+val_n]\n",
    "            test = self.X[train_n+val_n:]\n",
    "            return train, val, test\n",
    "        \n",
    "        else:\n",
    "            trainsize =  1 - testsize\n",
    "            train_n = round(self.X.shape[0] * trainsize)\n",
    "            train = self.X[:train_n]\n",
    "            test = self.X[train_n:]\n",
    "            return train, test\n",
    "    \n",
    "\n",
    "    def name(self):\n",
    "        \"\"\"\n",
    "            Assigns named attributes to specific datasets\n",
    "        \"\"\"\n",
    "        self.X_train = self.datasets[0]\n",
    "        self.y_train = self.labels[0]\n",
    "        if len(self.datasets) > 2:\n",
    "            self.X_val = self.datasets[1]\n",
    "            self.y_val = self.labels[1]\n",
    "            self.X_test = self.datasets[2]\n",
    "            self.y_test = self.labels[2]\n",
    "\n",
    "        else:\n",
    "            self.X_test = self.datasets[1]\n",
    "            self.y_test = self.labels[1]\n",
    "\n",
    "\n",
    "    def batch(self, dataset, labels, batch_size:int=1):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        num_batches = len(dataset) // batch_size\n",
    "        if len(dataset) % batch_size != 0:\n",
    "            num_batches += 1\n",
    "\n",
    "        batches = []\n",
    "        for i in range(num_batches):\n",
    "            start_index = i * batch_size\n",
    "            end_index = (i + 1) * batch_size\n",
    "            batch = (dataset[start_index:end_index], labels[start_index:end_index])\n",
    "            batches.append(batch)\n",
    "        \n",
    "        return batches\n",
    "\n",
    "\n",
    "seeds = EmbeddingDataSet('data/seeds.data')\n",
    "seeds.load(seed=1, validation=True, valsize=.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Network\n",
      "168 training batches\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(45)\n",
    "\n",
    "class FFMLP(object):\n",
    "    \n",
    "    \"\"\"\n",
    "        Feed-Forward Multi Layer Perceptron for Classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        epochs:int,\n",
    "        batch_size:int,\n",
    "        features:int,\n",
    "        output_nodes:int=1,\n",
    "        hidden_layers:int=1, \n",
    "        hidden_nodes:int=16, \n",
    "        init_weight:float=1,\n",
    "        alpha:float=.01):\n",
    "        \"\"\"\n",
    "            initializes a Feed Forward Multilayer Perceptron\n",
    "            for classification tasks\n",
    "            \n",
    "            x is an numpy array of training data\n",
    "            output_nodes is the number of classes\n",
    "        \"\"\"\n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.params = {\n",
    "            \"epochs\" : epochs,\n",
    "            \"hidden layers\": hidden_layers,\n",
    "            \"hidden nodes\": hidden_nodes,\n",
    "            \"initialization weight\": init_weight,\n",
    "            \"learning rate\": alpha,\n",
    "        }\n",
    "        \n",
    "        self.hidden_layer = {\n",
    "            \"weights\": np.random.uniform(low=0, high=init_weight, size=(features, hidden_nodes)),\n",
    "            \"bias\": np.random.uniform(low=0, high=init_weight, size=hidden_nodes)\n",
    "        }\n",
    "\n",
    "        if hidden_layers > 1:\n",
    "            self.hidden_layers = [\n",
    "                {\n",
    "                    \"weights\": np.random.uniform(low=0, high=init_weight, size=(hidden_nodes, hidden_nodes)),\n",
    "                    \"bias\": np.random.uniform(low=0, high=init_weight, size=hidden_nodes)\n",
    "                } for layer in range(hidden_layers-1)\n",
    "            ]\n",
    "        else:\n",
    "            self.hidden_layers = []\n",
    "\n",
    "        self.output_layer = {\n",
    "            \"weights\": np.random.uniform(low=0, high=init_weight, size=(hidden_nodes, output_nodes)),\n",
    "            \"bias\": np.random.uniform(low=0, high=init_weight, size=output_nodes)\n",
    "        }\n",
    "\n",
    "        self.network = dict() # a data structure to hold our layers\n",
    "        self.network['hidden layer 1'] = self.hidden_layer\n",
    "        for idx, layer in enumerate(self.hidden_layers, start=2):\n",
    "            self.network[f'hidden layer {idx}'] = layer        \n",
    "        self.network['output layer'] = self.output_layer # hidden to output\n",
    "        print('Initialized Network')\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Parameters: {self.params}\\n, Network: {self.network}'\n",
    "        \n",
    "    def ReLU(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "    def softmax_stable(self, X):\n",
    "        \"\"\"\n",
    "            stable softmax\n",
    "        \"\"\"\n",
    "        exps = np.exp(X - np.max(X))\n",
    "        return exps / np.sum(exps)\n",
    "    \n",
    "\n",
    "    def cross_entropy(self, X, y):\n",
    "        \"\"\"\n",
    "        X is the output from fully connected layer (num_examples x num_classes)\n",
    "          after softmax()\n",
    "        y is labels (num_examples x 1)\n",
    "            Note that y is not one-hot encoded vector. \n",
    "            It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.\n",
    "        \"\"\"\n",
    "        p = X\n",
    "        m = y.shape[0]\n",
    "        # takes the log of the highest values in p (which is the probability of the class)\n",
    "        log_likelihood = -np.log(p[range(m), y]) \n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "\n",
    "    def delta_cross_entropy(self, X, y):\n",
    "        \"\"\"\n",
    "        X is the output from fully connected layer (num_examples x num_classes)\n",
    "        y is labels (num_examples x 1)\n",
    "            Note that y is not one-hot encoded vector. \n",
    "            It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        X[range(m), y] -= 1\n",
    "        X = X / m\n",
    "        return X\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.z = batch[0]\n",
    "        for idx, (key, value) in enumerate(self.network.items(), start=1):\n",
    "            print(f\"forward {key}\")\n",
    "            self.network[key][f'z{idx}'] = self.z\n",
    "            self.z = np.dot(self.z, value['weights']) + value['bias']\n",
    "            a = self.ReLU(self.z) # activations\n",
    "            self.network[key][f'a{idx}'] = a\n",
    "            \n",
    "        self.outputs = np.apply_along_axis(self.softmax_stable, 1, a)\n",
    "        self.loss = self.cross_entropy(self.outputs, batch[1])\n",
    "        return self.loss\n",
    "\n",
    "\n",
    "    def backward(self, batch):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        delta = self.delta_cross_entropy(self.outputs, batch[1])\n",
    "        for key in reversed(list(self.network.keys())):\n",
    "\n",
    "            layer = self.network[key]\n",
    "            input_data = layer['input']\n",
    "            output_data = layer['output']\n",
    "            weights = layer['weights']\n",
    "            bias = layer['bias']\n",
    "            \n",
    "            d_weights = np.dot(input_data.T, delta)\n",
    "            d_bias = np.sum(delta, axis=0)\n",
    "            \n",
    "            self.network[key]['weights'] = d_weights\n",
    "            self.network[key]['bias'] = d_bias\n",
    "            \n",
    "            delta = np.dot(delta, weights.T) * output_data * (1 - output_data)\n",
    "            \n",
    "        self.output_layer['weights'] = np.dot(self.network['hidden layer 1']['output'].T, delta)\n",
    "        self.output_layer['bias'] = np.sum(delta, axis=0)\n",
    "\n",
    "\n",
    "    def fit(self, batches):\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch in batches:\n",
    "                pass_loss = self.forward(batch)\n",
    "                self.backward()\n",
    "                # Print loss every 10 epochs\n",
    "                if epoch % 1 == 0:\n",
    "                    print(f\"Epoch {epoch}, loss: {pass_loss:.4f}\")\n",
    "\n",
    "\n",
    "# Multi Layer Perceptron\n",
    "EPOCHS = 10\n",
    "CLASSES = 3\n",
    "HIDDEN_NODES = 4\n",
    "HIDDEN_LAYERS = 1\n",
    "INITWEIGHT = .1\n",
    "ALPHA = .10\n",
    "\n",
    "network = FFMLP(\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=1,\n",
    "    features=2,\n",
    "    hidden_layers = HIDDEN_LAYERS,\n",
    "    hidden_nodes = HIDDEN_NODES,\n",
    "    output_nodes = CLASSES,\n",
    "    init_weight=INITWEIGHT,\n",
    "    alpha=ALPHA\n",
    ")\n",
    "\n",
    "train_batches = seeds.batch(seeds.X_train, seeds.y_train, 1)\n",
    "print(f\"{len(train_batches)} training batches\")\n",
    "#print(f\"pass loss: {network.forward(train_batches[0][0][:)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden layer 1': {'weights': array([[0.09890115, 0.05495447, 0.02814473, 0.00772896],\n",
       "         [0.04444695, 0.0472808 , 0.0048522 , 0.01633244]]),\n",
       "  'bias': array([0.01159507, 0.06273917, 0.0856182 , 0.06501024])},\n",
       " 'output layer': {'weights': array([[0.09907217, 0.04703507, 0.06182945],\n",
       "         [0.02826672, 0.09760033, 0.0673068 ],\n",
       "         [0.04405309, 0.02896873, 0.05096997],\n",
       "         [0.01124609, 0.02269548, 0.04785523]]),\n",
       "  'bias': array([0.02427582, 0.03879825, 0.08188734])}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_batches[0][0][:, :2].round(2)\n",
    "W = network.network['hidden layer 1']['weights'].round(2)\n",
    "b = network.network['hidden layer 1']['bias'].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1 , 0.05, 0.03, 0.01],\n",
       "       [0.04, 0.05, 0.  , 0.02]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0968, 0.0685, 0.021 , 0.0204]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(x, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.95346953]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the input data\n",
    "X = np.array([[1, 2]])\n",
    "\n",
    "# Define the weights and biases\n",
    "# Hidden Layer\n",
    "W1 = np.array([[0.1, 0.2], [.30, .40]])\n",
    "b1 = np.array([0.5, 0.6])\n",
    "\n",
    "# Output layer\n",
    "W2 = np.array([[0.7], [0.8]])\n",
    "b2 = np.array([0.9])\n",
    "\n",
    "# Perform the forward pass\n",
    "z1 = np.dot(X, W1) + b1\n",
    "a1 = np.maximum(z1, 0)  # ReLU activation\n",
    "z2 = np.dot(a1, W2) + b2\n",
    "\n",
    "y_pred = 1 / (1 + np.exp(-z2))  # Sigmoid activation\n",
    "y_pred\n",
    "\n",
    "# exps = np.exp(z2 - np.max(z2))\n",
    "# y_pred = exps / np.sum(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.06764782]]\n"
     ]
    }
   ],
   "source": [
    "# Define the target output\n",
    "y_true = np.array([[0]])\n",
    "\n",
    "# Compute the loss using binary cross-entropy\n",
    "loss = - y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
    "#loss = - sum(y_true * np.log(y_pred))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_cross_entropy(X, y):\n",
    "    \"\"\"\n",
    "    X is the output from fully connected layer (num_examples x num_classes)\n",
    "    y is labels (num_examples x 1)\n",
    "        Note that y is not one-hot encoded vector. \n",
    "        It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    X[range(m), y] -= 1\n",
    "    X = X / m\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the gradient of the loss with respect to the output\n",
    "dL_dy_pred = -(y_true / y_pred) + (1 - y_true) / (1 - y_pred)\n",
    "\n",
    "\n",
    "# Compute the gradient of the loss with respect to the parameters of the second layer\n",
    "dy_pred_dz2 = y_pred * (1 - y_pred)\n",
    "dz2_da1 = W2\n",
    "dz2_db2 = np.ones((1, 1))\n",
    "dL_dz2 = dL_dy_pred * dy_pred_dz2\n",
    "dL_dW2 = np.dot(a1.T, dL_dz2)\n",
    "dL_db2 = np.sum(dL_dz2, axis=0)\n",
    "\n",
    "# Compute the gradient of the loss with respect to the parameters of the first layer\n",
    "dz2_da1 = W2\n",
    "da1_dz1 = np.where(z1 > 0, 1, 0)\n",
    "dz1_dW1 = X\n",
    "dz1_db1 = np.ones((1, 2))\n",
    "dL_dz1 = np.dot(dL_dz2, dz2_da1.T) * da1_dz1\n",
    "dL_dW1 = np.dot(dz1_dW1.T, dL_dz1)\n",
    "dL_db1 = np.sum(dL_dz1, axis=0)\n",
    "\n",
    "# Update the parameters using gradient descent\n",
    "learning_rate = 0.1\n",
    "W2 -= learning_rate * dL_dW2\n",
    "b2 -= learning_rate * dL_db2\n",
    "\n",
    "W1 -= learning_rate * dL_dW1\n",
    "b1 -= learning_rate * dL_db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03325713, 0.12372244],\n",
       "       [0.16651427, 0.24744488]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.34478128, 0.32202362, 0.3331951 ]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the input data\n",
    "X = train_batches[0][0]\n",
    "\n",
    "# Define the weights and biases\n",
    "# Hidden Layer\n",
    "W1 = network.network['hidden layer 1']['weights']\n",
    "b1 = network.network['hidden layer 1']['bias']\n",
    "\n",
    "# Hidden Layer\n",
    "W2 = network.network['hidden layer 2']['weights']\n",
    "b2 = network.network['hidden layer 2']['bias']\n",
    "\n",
    "# Hidden Layer\n",
    "W3 = network.network['hidden layer 3']['weights']\n",
    "b3 = network.network['hidden layer 3']['bias']\n",
    "\n",
    "# Output Layer\n",
    "W4 = network.network['output layer']['weights']\n",
    "b4 = network.network['output layer']['bias']\n",
    "\n",
    "\n",
    "# Perform the forward pass\n",
    "z1 = np.dot(X, W1) + b1\n",
    "a1 = np.maximum(z1, 0)  # ReLU activation\n",
    "\n",
    "z2 = np.dot(a1, W2) + b2\n",
    "a2 = np.maximum(z2, 0)\n",
    "\n",
    "z3 = np.dot(a2, W3) + b3\n",
    "a3 = np.maximum(z3, 0)\n",
    "\n",
    "z4 = np.dot(a3, W4) + b4\n",
    "a4 = np.maximum(z4, 0)\n",
    "\n",
    "\n",
    "def softmax_stable(X):\n",
    "    \"\"\"\n",
    "        stable softmax\n",
    "    \"\"\"\n",
    "    exps = np.exp(X - np.max(X))\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "\n",
    "y_preds = softmax_stable(a4)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.array([[0]])# train_batches[0][1] \n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.064845023445294"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy(X, y):\n",
    "    \"\"\"\n",
    "    X is the output from fully connected layer (num_examples x num_classes)\n",
    "      after softmax()\n",
    "    y is labels (num_examples x 1)\n",
    "        Note that y is not one-hot encoded vector. \n",
    "        It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.\n",
    "    \"\"\"\n",
    "    p = X\n",
    "    m = y.shape[0]\n",
    "    log_likelihood = -np.log(p[range(m), y]) \n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss\n",
    "\n",
    "loss = cross_entropy(y_preds, y_true)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,3) and (16,16) not aligned: 3 (dim 1) != 16 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[157], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m dz1_dW1 \u001b[39m=\u001b[39m X\n\u001b[1;32m     25\u001b[0m \u001b[39m#dz1_db1 = np.ones((1, 2))\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m dL_dz1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(dL_dz2, dz2_da1\u001b[39m.\u001b[39;49mT)\u001b[39m# * da1_dz1\u001b[39;00m\n\u001b[1;32m     27\u001b[0m dL_dW1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(dz1_dW1\u001b[39m.\u001b[39mT, dL_dz1)\n\u001b[1;32m     28\u001b[0m dL_db1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(dL_dz1, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,3) and (16,16) not aligned: 3 (dim 1) != 16 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Compute the gradient of the loss with respect to the output\n",
    "dL_dy_pred = delta_cross_entropy(y_preds, y_true)\n",
    "\n",
    "# Compute the gradient of the loss with respect to the parameters of the third layer\n",
    "dy_pred_dz3 = y_pred * (1 - y_pred)\n",
    "dz2_da2 = W3\n",
    "dz2_db3 = np.ones((1, 1))\n",
    "dL_dz3 = dL_dy_pred * dy_pred_dz3\n",
    "dL_dW3 = np.dot(a2.T, dL_dz3)\n",
    "dL_db3 = np.sum(dL_dz3, axis=0)\n",
    "\n",
    "\n",
    "# Compute the gradient of the loss with respect to the parameters of the second layer\n",
    "dy_pred_dz2 = y_pred * (1 - y_pred)\n",
    "dz2_da1 = W2\n",
    "dz2_db2 = np.ones((1, 1))\n",
    "dL_dz2 = dL_dy_pred * dy_pred_dz2\n",
    "dL_dW2 = np.dot(a1.T, dL_dz2)\n",
    "dL_db2 = np.sum(dL_dz2, axis=0)\n",
    "\n",
    "# Compute the gradient of the loss with respect to the parameters of the first layer\n",
    "dz2_da1 = W2\n",
    "da1_dz1 = np.where(z1 > 0, 1, 0)\n",
    "dz1_dW1 = X\n",
    "#dz1_db1 = np.ones((1, 2))\n",
    "dL_dz1 = np.dot(dL_dz2, dz2_da1.T)# * da1_dz1\n",
    "dL_dW1 = np.dot(dz1_dW1.T, dL_dz1)\n",
    "dL_db1 = np.sum(dL_dz1, axis=0)\n",
    "\n",
    "# # Update the parameters using gradient descent\n",
    "# learning_rate = 0.1\n",
    "# W2 -= learning_rate * dL_dW2\n",
    "# b2 -= learning_rate * dL_db2\n",
    "\n",
    "# W1 -= learning_rate * dL_dW1\n",
    "# b1 -= learning_rate * dL_db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "input_size = 7\n",
    "hidden_size1 = 16\n",
    "hidden_size2 = 16\n",
    "output_size = 3\n",
    "\n",
    "# Initialize the weights and biases\n",
    "W1 = np.random.randn(input_size, hidden_size1)\n",
    "b1 = np.zeros((1, hidden_size1))\n",
    "W2 = np.random.randn(hidden_size1, hidden_size2)\n",
    "b2 = np.zeros((1, hidden_size2))\n",
    "W3 = np.random.randn(hidden_size2, output_size)\n",
    "b3 = np.zeros((1, output_size))\n",
    "\n",
    "# Define the activation function (sigmoid in this case)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the derivative of the activation function\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Define the forward pass function\n",
    "def forward(X):\n",
    "    # Calculate the weighted sum of the inputs for the first hidden layer\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    # Apply the activation function to the output of the first hidden layer\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    # Calculate the weighted sum of the inputs for the second hidden layer\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    # Apply the activation function to the output of the second hidden layer\n",
    "    a2 = sigmoid(z2)\n",
    "    # Calculate the weighted sum of the inputs for the output layer\n",
    "    z3 = np.dot(a2, W3) + b3\n",
    "    # Apply the softmax activation function to the output of the output layer\n",
    "    exp_scores = np.exp(z3)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return a1, a2, probs\n",
    "\n",
    "# Define the loss function (cross-entropy in this case)\n",
    "def cross_entropy_loss(probs, y):\n",
    "    num_examples = len(y)\n",
    "    corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    return 1./num_examples * data_loss\n",
    "\n",
    "# Define the derivative of the loss function with respect to the output layer\n",
    "def delta_cross_entropy_loss(probs, y):\n",
    "    num_examples = len(y)\n",
    "    delta = probs\n",
    "    delta[range(num_examples),y] -= 1\n",
    "    return delta\n",
    "\n",
    "\n",
    "def cross_entropy(self, X, y):\n",
    "    \"\"\"\n",
    "    X is the output from fully connected layer (num_examples x num_classes)\n",
    "        after softmax()\n",
    "    y is labels (num_examples x 1)\n",
    "        Note that y is not one-hot encoded vector. \n",
    "        It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    p = X\n",
    "    \n",
    "    # takes the log of the highest values in p (which is the probability of the class)\n",
    "    log_likelihood = -np.log(p[range(m), y]) \n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss\n",
    "\n",
    "def delta_cross_entropy(self, X, y):\n",
    "    \"\"\"\n",
    "    X is the output from fully connected layer (num_examples x num_classes)\n",
    "    y is labels (num_examples x 1)\n",
    "        Note that y is not one-hot encoded vector. \n",
    "        It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    X[range(m), y] -= 1\n",
    "    X = X / m\n",
    "    return X\n",
    "\n",
    "\n",
    "# Define the backward pass function\n",
    "def backward(X, y, a1, a2, probs):\n",
    "    # Calculate the derivative of the loss function with respect to the output layer\n",
    "    delta3 = delta_cross_entropy_loss(probs, y)\n",
    "    # Calculate the derivative of the loss function with respect to the weights and biases in the output layer\n",
    "    dW3 = np.dot(a2.T, delta3)\n",
    "    db3 = np.sum(delta3, axis=0)\n",
    "    # Calculate the derivative of the loss function with respect to the second hidden layer\n",
    "    delta2 = np.dot(delta3, W3.T) * sigmoid_derivative(a2)\n",
    "    # Calculate the derivative of the loss function with respect to the weights and biases in the second hidden layer\n",
    "    dW2 = np.dot(a1.T, delta2)\n",
    "    db2 = np.sum(delta2, axis=0)\n",
    "    # Calculate the derivative of the loss function with respect to the first hidden layer\n",
    "    delta1 = np.dot(delta2, W2.T) * sigmoid_derivative(a1)\n",
    "    # Calculate the derivative of the loss function with respect to the\n",
    "    # weights and biases in the first hidden layer\n",
    "    dW1 = np.dot(X.T, delta1)\n",
    "    db1 = np.sum(delta1, axis=0)\n",
    "    return dW1, db1, dW2, db2, dW3, db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, learning_rate, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Perform the forward pass to get the predicted probabilities\n",
    "        a1, a2, probs = forward(X)\n",
    "        # Calculate the loss\n",
    "        loss = cross_entropy_loss(probs, y)\n",
    "        # Perform the backward pass to get the gradients\n",
    "        dW1, db1, dW2, db2, dW3, db3 = backward(X, y, a1, a2, probs)\n",
    "        # Update the weights and biases using gradient descent\n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "        W3 -= learning_rate * dW3\n",
    "        b3 -= learning_rate * db3\n",
    "        # Print the loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, loss: {loss}\")\n",
    "        return W1, b1, W2, b2, W3, b3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some random data for training and testing\n",
    "\n",
    "X_train = np.random.randn(100, input_size)\n",
    "y_train = np.random.randint(output_size, size=100)\n",
    "X_test = np.random.randn(20, input_size)\n",
    "y_test = np.random.randint(output_size, size=20)\n",
    "\n",
    "Train the neural network using the training data\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_epochs = 1000\n",
    "W1, b1, W2, b2, W3, b3 = train(X_train, y_train, learning_rate, num_epochs)\n",
    "\n",
    "Evaluate the neural network using the testing data\n",
    "\n",
    "a1, a2, probs = forward(X_test)\n",
    "predictions = np.argmax(probs, axis=1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Test accuracy: {accuracy}\")\n",
    "\n",
    "csharp\n",
    "Copy code\n",
    "\n",
    "Note that this code is just an example and may not work out-of-the-box for your specific use case. You may need to tweak the architecture, hyperparameters, and other aspects of the code to get the best results for your problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('origami_datasci-2ZMWArRq')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb8348daee5f03dddbbbd8761b0688d0a17b168f6df1d4bcf8481e2ef54f7e76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
